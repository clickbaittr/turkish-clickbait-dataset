{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebE2hQFSiOED",
        "colab_type": "text"
      },
      "source": [
        "**Long-Short Term Memory for Clickbait Detection in ClickbaitTR**\n",
        "\n",
        "---\n",
        "\n",
        "The data is received from the preprocessing stage. As is explained in the paper, there are two types of input determined for detecting a clickbait tweet. The first type of information is received from the text, whereas the second type of information is received from the special characters and text-based features such as uppercase count, word length and tweet length of tweets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bU_iEpWzwmn",
        "colab_type": "code",
        "outputId": "522cfe9e-21c3-4c14-c4d8-ff30de73e31e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "!git clone -v https://github.com/ahmetax/kalbur.git "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kalbur'...\n",
            "POST git-upload-pack (165 bytes)\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Total 205 (delta 0), reused 0 (delta 0), pack-reused 205\u001b[K\n",
            "Receiving objects: 100% (205/205), 1.24 MiB | 1.15 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KIizm6Ta_lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocessing as pr\n",
        "import sys\n",
        "\n",
        "pr.current_path = \"/content/\"\n",
        "\n",
        "with open(pr.current_path + 'kalbur/kelime_bol.py', 'r') as file :\n",
        "  filedata = file.read()\n",
        "\n",
        "filedata = filedata.replace('veri/', pr.current_path + \"kalbur/veri/\")\n",
        "\n",
        "with open(pr.current_path + 'kalbur/kelime_bol.py', 'w') as file:\n",
        "  file.write(filedata)\n",
        "\n",
        "sys.path.append(pr.current_path + \"kalbur/\")\n",
        "\n",
        "import kelime_bol as kb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GSsiIXe0d5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_of_samples = 1000 # toy example\n",
        "\n",
        "csv_files = {\"limon\":\"dataset/limon_clickbait.csv\",\n",
        "             \"evrensel\":\"dataset/evrensel_non-clickbait.csv\",\n",
        "             \"spoiler\":\"dataset/spoiler_clickbait.csv\",\n",
        "             \"diken\":\"dataset/diken_non-clickbait.csv\"}\n",
        "\n",
        "clickbait, non_clickbait = pr.return_data(csv_files)\n",
        "\n",
        "special_characters = [\"#\", \"?\", \"!\", \".\", \"@\"]\n",
        "\n",
        "words_will_be_removed = [\"işçi\", \"eylem\", \"meteoroloji\", \"katliam\", \n",
        "                          \"murat\", \"altı\", \"seçim\", \"diren\", \"dev\", \n",
        "                          \"gazze\", \"blog\", \"protesto\", \"beş\", \n",
        "                          \"yaşam\", \"manşet\", \"günaydın\", \"türkiye\", \n",
        "                          \"sınır\",\"chp\", \"grev\", \"yaralı\", \"ateşkes\", \"yazı\", \"maden\", \"bayi\"]\n",
        "\n",
        "X_train, Xsc_train, Y_train, X_test, Xsc_test, Y_test, unique_word_list = pr.generatesample(clickbait[:no_of_samples], non_clickbait[:no_of_samples], \n",
        "                                                                   special_characters, words_will_be_removed, \n",
        "                                                                   isseparate=True, scaling=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSDVqOHqcE5q",
        "colab_type": "text"
      },
      "source": [
        "LSTM network receives the full text of tweets after preprocessing as a first input and receives information about special characters, uppercase letters and number of letters and words as a second input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaIvRMdbUZRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Dense, Embedding, LSTM, concatenate, CuDNNLSTM\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=2)\n",
        "\n",
        "max_tweet_length = len(unique_word_list)\n",
        "text_input = Input(shape=(None,), name='text_input')\n",
        "x = Embedding(input_dim = max_tweet_length + 1, output_dim=3)(text_input)\n",
        "lstm_out = LSTM(2)(x)\n",
        "\n",
        "special_characters = Input(shape=(9, ), name='special_input')\n",
        "b = Dense(2)(special_characters)\n",
        "merged = concatenate([lstm_out, b])\n",
        "\n",
        "output = Dense(1, activation='sigmoid', name='output')(merged)\n",
        "\n",
        "model = Model(inputs=[text_input, special_characters], outputs=output)\n",
        "adam = optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "plot_model(model, to_file='model.png')\n",
        "\n",
        "history = model.fit({'text_input': X_train, 'special_input': Xsc_train},\n",
        "          {'output': Y_train}, validation_data = [{'text_input': X_test, 'special_input': Xsc_test},\n",
        "          {'output': Y_test}],\n",
        "          epochs=32, \n",
        "          batch_size=128,\n",
        "          callbacks=[es])\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "csfont = {'fontname':'Arial'}\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.rcParams['font.size'] = 30\n",
        "plt.plot(history.history['acc'], linewidth=4.0)\n",
        "plt.plot(history.history['val_acc'], linewidth=4.0)\n",
        "plt.title('Model accuracy', **csfont)\n",
        "plt.ylabel('Accuracy', **csfont)\n",
        "plt.xlabel('Epoch', **csfont)\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "plt.savefig('LSTM_graph.png', bbox_inches = 'tight')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.plot(history.history['loss'], linewidth=4.0)\n",
        "plt.plot(history.history['val_loss'], linewidth=4.0)\n",
        "plt.title('Model loss', **csfont)\n",
        "plt.ylabel('Loss', **csfont)\n",
        "plt.xlabel('Epoch', **csfont)\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.rcParams['font.size'] = 30\n",
        "plt.show()\n",
        "plt.savefig('LSTM_loss_graph.png', bbox_inches = 'tight')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}