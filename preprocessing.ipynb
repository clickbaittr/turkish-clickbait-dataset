{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w3rRGLvNp8q",
        "colab_type": "text"
      },
      "source": [
        "**Downloading modules**\n",
        "\n",
        "In this part of the code, the kalbur module, which is used for finding roots of turkish words, is downloaded from github. This module is a necessity for running the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKfgIIF3xYvS",
        "colab_type": "code",
        "outputId": "32704d5c-d175-4397-93be-dc9ce2cfe2ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "!git clone -v https://github.com/ahmetax/kalbur.git "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kalbur'...\n",
            "POST git-upload-pack (165 bytes)\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Total 205 (delta 0), reused 0 (delta 0), pack-reused 205\u001b[K\n",
            "Receiving objects: 100% (205/205), 1.24 MiB | 16.03 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kK1wT5Ai_By",
        "colab_type": "code",
        "outputId": "9e4bd0f1-cab6-4225-e811-f8ec8518e791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "current_path = '/content/'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import scale\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFqnJd8vSN21",
        "colab_type": "text"
      },
      "source": [
        "The kalbur module returns error because of absolute path defined in the .py script. For this reason, the path in the module named kok_tara(..) in the python script named kelime_bol.py is changed to be current_path/veri/KOKOZLER.txt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4udiHRmPxgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(current_path + 'kalbur/kelime_bol.py', 'r') as file :\n",
        "  filedata = file.read()\n",
        "\n",
        "if \"kalbur/veri/\" not in filedata:\n",
        "  filedata = filedata.replace('veri/', current_path + \"kalbur/veri/\")\n",
        "\n",
        "  with open(current_path + 'kalbur/kelime_bol.py', 'w') as file:\n",
        "    file.write(filedata)\n",
        "\n",
        "sys.path.append(current_path + \"kalbur/\")\n",
        "import kelime_bol as kb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpHzQTKPSSCt",
        "colab_type": "text"
      },
      "source": [
        "**Data returning and preprocessing**\n",
        "\n",
        "The functions defined below are used to return and preprocess clickbait and non-clickbait tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfMJGDyTSlWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_data(csv_files):\n",
        "  limon = pd.read_csv(csv_files[\"limon\"], encoding=\"utf-8-sig\", skip_blank_lines=True).dropna()\n",
        "  print(\"# of tweets in limon:\", len(limon))\n",
        "  evrensel = pd.read_csv(csv_files[\"evrensel\"], encoding=\"utf-8-sig\", skip_blank_lines=True).dropna()\n",
        "  print(\"# of tweets in evrensel:\", len(evrensel))\n",
        "  spoiler = pd.read_csv(csv_files[\"spoiler\"], encoding=\"utf-8-sig\", skip_blank_lines=True).dropna()\n",
        "  print(\"# of tweets in spoiler:\", len(spoiler))\n",
        "  diken = pd.read_csv(csv_files[\"diken\"], encoding=\"utf-8-sig\", skip_blank_lines=True).dropna()\n",
        "  print(\"# of tweets in diken:\", len(diken))\n",
        "  return limon[\"full_text\"].to_list() + spoiler[\"full_text\"].to_list(), evrensel[\"full_text\"].to_list() + diken[\"full_text\"].to_list()\n",
        "\n",
        "def stemmingStep(data):\n",
        "  sentencelist = []\n",
        "  tweetlist = []\n",
        "  for i in data:\n",
        "    for j in i:\n",
        "      if len(kb.kok_tara(j)[1]) == 0 and not len(j) == 0:\n",
        "        if \"'\" in j:\n",
        "          j = j.split(\"'\")\n",
        "          if j[0].isalpha():\n",
        "            sentencelist.append(j[0])\n",
        "        elif \"’\" in j:\n",
        "          j = j.split(\"'\")\n",
        "          if j[0].isalpha():\n",
        "            sentencelist.append(j[0])\n",
        "      else:\n",
        "        if j.isalpha() and not len(j) == 0:\n",
        "          sentencelist.append(kb.kok_tara(j)[1])\n",
        "    tweetlist.append(sentencelist)\n",
        "    sentencelist = []\n",
        "  return tweetlist\n",
        "\n",
        "def count_special_characters(tweetlist, special_characters):\n",
        "  ntweetlist = []\n",
        "  special_characters_in_the_tweetlist = []\n",
        "  other_special_characters_in_the_tweetlist = []\n",
        "  uppercase_characters_in_the_tweetlist = []\n",
        "  for idx, tweet in enumerate(tweetlist):\n",
        "    if not tweet:\n",
        "      special_characters_in_the_tweet = [0]*len(special_characters)\n",
        "      other_special_characters_in_the_tweet = 0\n",
        "      uppercase_characters_in_the_tweet = 0\n",
        "      special_characters_in_the_tweetlist.append(np.array(special_characters_in_the_tweet))\n",
        "      other_special_characters_in_the_tweetlist.append(other_special_characters_in_the_tweet)\n",
        "      uppercase_characters_in_the_tweetlist.append(uppercase_characters_in_the_tweet)\n",
        "      ntweetlist.append(tweet)\n",
        "      continue\n",
        "    special_characters_in_the_tweet = []\n",
        "    other_special_characters_in_the_tweet = 0\n",
        "    uppercase_characters_in_the_tweet = 0\n",
        "    new_tweet = []\n",
        "    for word in tweet:\n",
        "      special_characters_count = []\n",
        "      for s in special_characters:\n",
        "        special_characters_count.append(word.count(s))\n",
        "        word = word.replace(s, \"\")\n",
        "      nword = \"\".join([l for l in word if l.isalnum() or l == \" \" or l == \"'\" or l == \"’\"])\n",
        "      special_characters_in_the_tweet.append(special_characters_count)\n",
        "      other_special_characters_in_the_tweet += len(word)- len(nword)\n",
        "      uppercase_characters_in_the_tweet += sum(1 if l.isupper() else 0 for l in nword)\n",
        "      new_tweet.append(nword.lower())\n",
        "    ntweetlist.append(new_tweet)\n",
        "    special_characters_in_the_tweetlist.append(np.sum(np.array(special_characters_in_the_tweet), axis=0))\n",
        "    other_special_characters_in_the_tweetlist.append(other_special_characters_in_the_tweet)\n",
        "    uppercase_characters_in_the_tweetlist.append(uppercase_characters_in_the_tweet)\n",
        "  return ntweetlist, special_characters_in_the_tweetlist, other_special_characters_in_the_tweetlist, uppercase_characters_in_the_tweetlist\n",
        "\n",
        "def calculate_average_tweet_length(tweetlist):\n",
        "  tweet_lengths = [len(tweet) if not len(tweet) == 0 else 0 for tweet in tweetlist]\n",
        "  return np.mean(tweet_lengths), tweet_lengths\n",
        "\n",
        "def calculate_average_word_length(tweetlist):\n",
        "  return [np.mean([len(w) for w in tweet]) if not len(tweet) == 0 else 0 for tweet in tweetlist]\n",
        "    \n",
        "def remove_selected_words(tweetlist, words_will_be_removed):\n",
        "  return [[w for w in wordlist if w not in words_will_be_removed] for wordlist in tweetlist]\n",
        "\n",
        "def wordspace(tweetlist):\n",
        "  return list(set(sum([[w for w in wordlist] for wordlist in tweetlist], [])))\n",
        "\n",
        "def coding_tweets(tweetlist, unique_word_list, word_indexes):\n",
        "  coded_tweets = []\n",
        "  for idx, tweet in enumerate(tweetlist):\n",
        "    coded_tweet = np.zeros((1, len(unique_word_list) + 2))\n",
        "    for indx in word_indexes[idx]:\n",
        "      coded_tweet[:, idx] += 1\n",
        "    coded_tweets.append(coded_tweet)\n",
        "  return np.squeeze(np.array(coded_tweets))\n",
        "\n",
        "def  generatesample(clickbait, non_clickbait, \n",
        "                    special_characters, words_will_be_removed, \n",
        "                    isseparate=False, scaling=True, \n",
        "                    for_data_generator=False):\n",
        "  \n",
        "  clickbait = [[w for w in c.split(\" \") if \"http\" not in w] for c in clickbait] # remove the last element, which is link\n",
        "  non_clickbait = [[w for w in c.split(\" \") if \"http\" not in w] for c in non_clickbait] # remove the last element, which is link\n",
        "\n",
        "  clickbait, sp_clickbait, osp_clickbait, up_clickbait = count_special_characters(clickbait, special_characters)\n",
        "  mean_clickbait, len_clickbait = calculate_average_tweet_length(clickbait)\n",
        "  word_mean_clickbait = calculate_average_word_length(clickbait)\n",
        "\n",
        "  non_clickbait, sp_non_clickbait, osp_non_clickbait, up_non_clickbait = count_special_characters(non_clickbait, special_characters)\n",
        "  mean_non_clickbait, len_non_clickbait = calculate_average_tweet_length(non_clickbait)\n",
        "  word_mean_non_clickbait = calculate_average_word_length(non_clickbait)\n",
        "\n",
        "  sample = clickbait + non_clickbait\n",
        "  sp_sample = sp_clickbait + sp_non_clickbait\n",
        "  osp_sample = osp_clickbait + osp_non_clickbait\n",
        "  up_sample = up_clickbait + up_non_clickbait\n",
        "  mean_sample = mean_clickbait + mean_non_clickbait\n",
        "  len_sample = len_clickbait + len_non_clickbait\n",
        "  word_mean = word_mean_clickbait + word_mean_non_clickbait\n",
        "\n",
        "  sample = remove_selected_words(stemmingStep(sample), words_will_be_removed)\n",
        "  unique_word_list = wordspace(sample)\n",
        "  word_indexes = [[unique_word_list.index(w) for w in tweet] for tweet in sample]\n",
        "\n",
        "  Xsc = np.c_[np.array(sp_sample), \n",
        "              np.array(osp_sample).reshape((-1, 1)), \n",
        "              np.array(up_sample).reshape((-1, 1)),\n",
        "              np.array(word_mean).reshape((-1, 1)),\n",
        "              np.array(len_sample).reshape((-1, 1))]\n",
        "  if scaling:\n",
        "    Xsc = scale(Xsc, axis=0)\n",
        "\n",
        "  Y = np.append(np.ones(len(clickbait)), np.zeros(len(non_clickbait)))\n",
        "\n",
        "  if for_data_generator:\n",
        "    shuffle_index = np.random.permutation(len(Xsc))\n",
        "    word_indexes = np.array(word_indexes)\n",
        "    Xsc = np.array(Xsc)\n",
        "    Y = np.array(Y)\n",
        "    return unique_word_list, word_indexes[shuffle_index], Xsc[shuffle_index], Y[shuffle_index]\n",
        "\n",
        "  X = coding_tweets(sample, unique_word_list, word_indexes)\n",
        "  sample_size = len(X)\n",
        "  shuffle_index = np.random.permutation(sample_size)\n",
        "\n",
        "  if isseparate:\n",
        "    X, Xsc, Y = X[shuffle_index], Xsc[shuffle_index], Y[shuffle_index]\n",
        "\n",
        "    X_test, Xsc_test, Y_test = X[:sample_size//5], Xsc[:sample_size//5], Y[:sample_size//5]\n",
        "    X_train, Xsc_train, Y_train = X[sample_size//5:], Xsc[sample_size//5:], Y[sample_size//5:]\n",
        "    return X_train, Xsc_train, Y_train, X_test, Xsc_test, Y_test, unique_word_list\n",
        "  else:\n",
        "    X = np.c_[X, Xsc]\n",
        "    X, Y = X[shuffle_index], Y[shuffle_index]\n",
        "    X_test, Y_test = X[:sample_size//5], Y[:sample_size//5]\n",
        "    X_train, Y_train = X[sample_size//5:], Y[sample_size//5:]\n",
        "    return X_train, Y_train, X_test, Y_test, unique_word_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcU2iQT9SjxG",
        "colab_type": "text"
      },
      "source": [
        "return_data(..) which returns the clickbait and non-clickbait data needs a dictionary as a parameter. The dictionary should include keys and respective file's paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZSYYjVEYmLr",
        "colab_type": "code",
        "outputId": "ad812f01-b8fd-49a7-ecbd-eb82b71b0282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "csv_files = {\"limon\":\"dataset/limon_clickbait.csv\",\n",
        "             \"evrensel\":\"dataset/evrensel_non-clickbait.csv\",\n",
        "             \"spoiler\":\"dataset/spoiler_clickbait.csv\",\n",
        "             \"diken\":\"dataset/diken_non-clickbait.csv\"}\n",
        "\n",
        "clickbait, non_clickbait = return_data(csv_files)\n",
        "\n",
        "print(\"# of clickbait tweets\", len(clickbait))\n",
        "print(\"# of non_clickbait tweets\", len(non_clickbait))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of tweets in limon: 21791\n",
            "# of tweets in evrensel: 13093\n",
            "# of tweets in spoiler: 1898\n",
            "# of tweets in diken: 10936\n",
            "# of clickbait tweets 23689\n",
            "# of non_clickbait tweets 24029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdmrLah_SypT",
        "colab_type": "text"
      },
      "source": [
        "As a toy example, we select 1000 tweets from clickbait and non-clickbait data. \n",
        "\n",
        "Additionally, one can define special_characters list for determining special characters that seem to be important for clickbait detection. For these special characters, a separate row is formed for all clickbait detection algorithms.\n",
        "\n",
        "Features extracted from a tweet:\n",
        "\n",
        "1.   words in the tweet \n",
        "2.   special characters [\"#\", \"?\", \"!\", \".\", \"@\"]\n",
        "3.   other special characters\n",
        "4.   number of uppercase letters\n",
        "5.   average word length\n",
        "6.   average tweet length\n",
        "\n",
        "\n",
        "Nine additional features to words in the tweet are determined.\n",
        "\n",
        "words_will_be_removed involves the suspected words that possibly help machine learning algorithms in deciding whether a tweet is clickbait or not. These words can be removed from the dataset so that models cannot exploit this problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK37rXj263LU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "special_characters = [\"#\", \"?\", \"!\", \".\", \"@\"]\n",
        "\n",
        "words_will_be_removed = [\"işçi\", \"eylem\", \"meteoroloji\", \"katliam\", \n",
        "                          \"murat\", \"altı\", \"seçim\", \"diren\", \"dev\", \n",
        "                          \"gazze\", \"blog\", \"protesto\", \"beş\", \n",
        "                          \"yaşam\", \"manşet\", \"günaydın\", \"türkiye\", \n",
        "                          \"sınır\",\"chp\", \"grev\", \"yaralı\", \"ateşkes\", \"yazı\", \"maden\", \"bayi\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s9g_mcCaFbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for toy example\n",
        "\n",
        "n_of_tweets = 1000\n",
        "                          \n",
        "X_train, Y_train, X_test, Y_test, unique_word_list = generatesample(clickbait, non_clickbait, \n",
        "                                                                   special_characters, words_will_be_removed, \n",
        "                                                                   isseparate=False, scaling=True, for_data_generator=False)\n",
        "\n",
        "print('# of unique words in the dataset', len(unique_word_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcWiCuHA4Z01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "def3426e-aaca-49fc-b5fa-59abfc4a52da"
      },
      "source": [
        "unique_word_list, word_indexes, Xsc, Y = generatesample(clickbait, non_clickbait, \n",
        "                                                        special_characters, words_will_be_removed, \n",
        "                                                        isseparate=False, scaling=True, for_data_generator=True)\n",
        "\n",
        "print('# of unique words in the dataset', len(unique_word_list))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of words: 81413\n",
            "1. clickbait data length after sp: 23689\n",
            "1. non_clickbait data length after sp: 24029\n",
            "# of words: 60103\n",
            "# of words: 60103\n",
            "# of words: 10912\n",
            "2. sample data length after sp: 47718\n",
            "# of unique words in the dataset 10912\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}